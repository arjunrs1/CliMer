{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'manual', 'table', 'hands']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))\n",
    "\n",
    "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "\n",
    "    return polygon\n",
    "\n",
    "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a polygon to a segmentation mask.\n",
    "\n",
    "    Args:\n",
    "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
    "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Segmentation mask with the polygon filled.\n",
    "    \"\"\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Convert polygon to an array of points\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "\n",
    "    # Fill the polygon with white color (255)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def load_image(image_str: str) -> Image.Image:\n",
    "    if image_str.startswith(\"http\"):\n",
    "        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_str).convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
    "    boxes = []\n",
    "    for result in results:\n",
    "        xyxy = result.box.xyxy\n",
    "        boxes.append(xyxy)\n",
    "\n",
    "    return [boxes]\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    masks = masks.cpu().float()\n",
    "    masks = masks.permute(0, 2, 3, 1)\n",
    "    masks = masks.mean(axis=-1)\n",
    "    masks = (masks > 0).int()\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    masks = list(masks)\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            shape = mask.shape\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            mask = polygon_to_mask(polygon, shape)\n",
    "            masks[idx] = mask\n",
    "\n",
    "    return masks\n",
    "\n",
    "def detect(\n",
    "    image: Image.Image,\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    detector_id: Optional[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
    "    object_detector = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
    "\n",
    "    labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
    "\n",
    "    results = object_detector(image,  candidate_labels=labels, threshold=threshold)\n",
    "    results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "def segment(\n",
    "    image: Image.Image,\n",
    "    detection_results: List[Dict[str, Any]],\n",
    "    polygon_refinement: bool = False,\n",
    "    segmenter_id: Optional[str] = None\n",
    ") -> List[DetectionResult]:\n",
    "    \"\"\"\n",
    "    Use Segment Anything (SAM) to generate masks given an image + a set of bounding boxes.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n",
    "\n",
    "    segmentator = AutoModelForMaskGeneration.from_pretrained(segmenter_id).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(segmenter_id)\n",
    "\n",
    "    boxes = get_boxes(detection_results)\n",
    "    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = segmentator(**inputs)\n",
    "    masks = processor.post_process_masks(\n",
    "        masks=outputs.pred_masks,\n",
    "        original_sizes=inputs.original_sizes,\n",
    "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "    )[0]\n",
    "\n",
    "    masks = refine_masks(masks, polygon_refinement)\n",
    "\n",
    "    for detection_result, mask in zip(detection_results, masks):\n",
    "        detection_result.mask = mask\n",
    "\n",
    "    return detection_results\n",
    "\n",
    "def compute_union_area(masks: List[np.ndarray]) -> int:\n",
    "    if not masks:\n",
    "        return 0\n",
    "    # Stack masks to create a 3D array and compute the logical OR across all masks\n",
    "    union_mask = np.logical_or.reduce(np.stack(masks, axis=0))\n",
    "    return np.sum(union_mask)\n",
    "\n",
    "def grounded_segmentation_area(\n",
    "    image: Union[Image.Image, str],\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    polygon_refinement: bool = False,\n",
    "    detector_id: Optional[str] = None,\n",
    "    segmenter_id: Optional[str] = None\n",
    ") -> int:\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "\n",
    "    detections = detect(image, labels, threshold, detector_id)\n",
    "    detections = segment(image, detections, polygon_refinement, segmenter_id)\n",
    "\n",
    "    # Extract masks from detections and compute the union area\n",
    "    masks = [detection.mask for detection in detections if detection.mask is not None]\n",
    "    return compute_union_area(masks)\n",
    "\n",
    "def sample_frames(start_frame, end_frame, num_samples=5):\n",
    "    return np.linspace(start_frame, end_frame, num=num_samples, dtype=int)\n",
    "\n",
    "def process_narr(take_name, cam_id, start_frame, end_frame, nouns, num_samples=5):\n",
    "    frame_numbers = sample_frames(start_frame, end_frame, num_samples)\n",
    "    union_areas = []\n",
    "\n",
    "    video_path = f'/datasets01/egoexo4d/v2/takes/{take_name}/frame_aligned_videos/{cam_id}.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for frame_number in frame_numbers:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)  # Set the current frame position\n",
    "        ret, frame_cv2 = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(f\"Failed to read frame {frame_number} from video {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert OpenCV frame to PIL Image\n",
    "        frame = cv2.cvtColor(frame_cv2, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        area = grounded_segmentation_area(\n",
    "            image=frame,\n",
    "            labels=nouns,\n",
    "            threshold=0.3,\n",
    "            polygon_refinement=True,\n",
    "            detector_id=\"IDEA-Research/grounding-dino-tiny\",\n",
    "            segmenter_id=\"facebook/sam-vit-base\"\n",
    "        )\n",
    "        union_areas.append(area)\n",
    "    cap.release()  # Release the video capture object\n",
    "    # Compute the mean of the union areas\n",
    "    mean_area = np.mean(union_areas) if union_areas else 0\n",
    "    return mean_area\n",
    "\n",
    "csv_file = '/private/home/arjunrs1/CliMer/data/egoexo4d/egoexo4d_egos_narrations_test.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "results = {}\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    video_name = row['video_id']\n",
    "    start_frame = row['start_frame']\n",
    "    end_frame = row['end_frame']\n",
    "    nouns = eval(row['noun_class'])\n",
    "    if \"hand\" not in nouns:\n",
    "        if \"hands\" not in nouns:\n",
    "            nouns.append(\"hand\")\n",
    "    take_name = video_name.rsplit(\"_\", 1)[0] if \"aria\" not in video_name else video_name.rsplit(\"_\", 2)[0]\n",
    "    cam_id = video_name.rsplit(\"_\", 1)[1] if \"aria\" not in video_name else \"_\".join(video_name.rsplit(\"_\", 2)[1:])\n",
    "    print(nouns)\n",
    "\n",
    "    mean_area = process_narr(take_name, cam_id, start_frame, end_frame, nouns)\n",
    "    results[video_name] = mean_area\n",
    "    break\n",
    "\n",
    "with open('gSAM_test_map.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
