{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import lmdb\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_token_counts(df, tokenizer, model, include_cls=True):\n",
    "    narrations = df['narration'].tolist()\n",
    "    encoded_input = tokenizer(narrations, padding='max_length', truncation=True, max_length=20, return_tensors='pt')\n",
    "    encoded_input = {key: val.to(model.device) for key, val in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    features = model_output.last_hidden_state\n",
    "    \n",
    "    # Calculate the number of tokens before padding\n",
    "    # Attention masks are 1 for real tokens and 0 for padding\n",
    "    if include_cls:\n",
    "        num_tokens = torch.sum(encoded_input['attention_mask'], dim=1).item()  # Includes [CLS]\n",
    "    else:\n",
    "        num_tokens = (torch.sum(encoded_input['attention_mask'], dim=1) - 1).item()  # Excludes [CLS]\n",
    "\n",
    "    return features, num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"/private/home/arjunrs1/CliMer/data/epic/EPIC_train.csv\"\n",
    "val_csv = \"/private/home/arjunrs1/CliMer/data/epic/EPIC_val.csv\"\n",
    "test_csv = \"/private/home/arjunrs1/CliMer/data/epic/EPIC_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(train_csv)\n",
    "data_val = pd.read_csv(val_csv)\n",
    "data_test = pd.read_csv(test_csv)\n",
    "data = pd.concat((data_train, data_val, data_test))\n",
    "#data = data[data.video_id.isin(['P01_01', 'P01_02'])] #For now, we have restricted to just these video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50167/50167 [16:26<00:00, 50.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path to LMDB database\n",
    "lmdb_path = '/private/home/arjunrs1/CliMer/lmdb_bert_features'\n",
    "\n",
    "# Create or open the LMDB database\n",
    "env = lmdb.open(lmdb_path, map_size=int(4e9))\n",
    "\n",
    "with env.begin(write=True) as txn:\n",
    "    for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        # Compute features for each narration\n",
    "        features, num_tokens = get_features_and_token_counts(pd.DataFrame([row]), tokenizer, model)\n",
    "        \n",
    "        # Prepare the data structure\n",
    "        to_save = {\n",
    "            'features': features.detach(),\n",
    "            'num_tokens': num_tokens\n",
    "        }\n",
    "        \n",
    "        # Serialize the data structure\n",
    "        serialized_data = pickle.dumps(to_save)\n",
    "        \n",
    "        # Put the serialized data in the database with the ASCII-encoded key\n",
    "        clip_id = row['narration_id'].encode('ascii')\n",
    "        txn.put(clip_id, serialized_data)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "        # Compute features for each narration\n",
    "        features, num_tokens = get_features_and_token_counts(pd.DataFrame([row]), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
